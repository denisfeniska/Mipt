{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c3b1c0-fa60-42af-8bbd-5efcc0368121",
   "metadata": {},
   "source": [
    "# Домашнее задание №3. Линейная регрессия. Расширение признакового пространства. Регуляризация. Валидация\n",
    "\n",
    "## Ф.И.О: _________\n",
    "\n",
    "### Описание.\n",
    "\n",
    "Домашнее задание состоит из 2-х частей:\n",
    "  - теоретическая часть\n",
    "  - практическая часть\n",
    "    - реализация модуля линейной регрессии\n",
    "    -  эксперименты\n",
    "\n",
    "**На проверку требуется отправить zip архив, который будет содержать следующие файлы:**\n",
    "  - модуль ``modules`` с реализованными классами\n",
    "  - заполненный блокнот в формате ``.ipynb``\n",
    "  - заполненный блокнот в формате ``.html`` **(в jupyter: File -> Save and Export Notebook As -> HTML -> ...)**\n",
    "\n",
    "-------------------------\n",
    "\n",
    "## Теоретическая часть. (4 points)\n",
    "\n",
    "**№1.** (2 point) Найдите субдифференциалы для следующих функций:\n",
    "- $f(x) = \\max(0, 1 − ax), \\ \\ a \\ — \\ const$,  во всех точках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957000de-6a7e-45b1-b772-ad0c1a7f344f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec017831-1f51-4bbf-a01f-935d2415d0ac",
   "metadata": {},
   "source": [
    "- $f(x) = \\sin x, \\ x \\in [0; \\frac{3}{2} \\pi]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e6bb2c-0ba5-486a-85c1-7c1b7bfdde1a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71e7a07f-98ac-4bf0-95c9-e49e314998f5",
   "metadata": {},
   "source": [
    "**№2.** (2 point) Рассмотрим задачу линейной регрессии с регуляризацией:\n",
    "\n",
    "$$L(w) = \\| Xw - y\\|_2^2 + \\lambda R(w)$$\n",
    "\n",
    "И попробуем разобраться, в каких случаях возникает случай вырожденного (нулевого решения) для LASSO и Ridge регрессии.\n",
    "\n",
    "- Покажите, что если оптимальное решение $w^*$ функционала $L(w) = \\| Xw - y\\|_2^2 + \\lambda \\|w\\|_1$ равно нулю, то выполняется неравенство $\\| 2 X^T y \\|_{\\infty} \\le \\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cacb732-a068-4754-8985-7534155d63e3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82fa2d61-7f9a-43a8-88c9-a99affcb3932",
   "metadata": {},
   "source": [
    "- Покажите, что для функционала $L(w) = \\| Xw - y\\|_2^2 + \\lambda \\| w \\|_2^2$ при $X^Ty \\neq 0$ не существует $\\lambda$ такого, что оптимальное решение $w^*$ обращается в нуль. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff54047-9a10-41cd-81ab-b791a5ac9a98",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16365f21-7017-40ef-a541-4a75f5e86ceb",
   "metadata": {},
   "source": [
    "------------------------\n",
    "\n",
    "## Практическая часть. (16 points)\n",
    "\n",
    "Данная часть задания направлена на ознакомление с линейными моделями и градиентными методами обучения. В задании необходимо:\n",
    "1. Написать на языке Python собственную реализацию модели линейной регрессии с произвольной функцией потерь и реализацию функции и градиента функции потерь для линейной регрессии. Реализации можно частично проверить через юнит-тесты (запускаются командой ``pytest tests.py``).\n",
    "2. Провести описанные ниже эксперименты с модельными данными и приложенным датасетом.\n",
    "3. Написать отчёт о проделанной работе (в формате jupyter notebook).\n",
    "\n",
    "### Реализация алгоритмов. (3 points)\n",
    "\n",
    "Везде под выборкой объектов будем понимать ``numpy.ndarray`` размера $N \\times D$ или разреженную матрицу ``scipy.sparse.csr_matrix`` того же размера, под ответами для объектов выборки будем понимать ``numpy.ndarray`` размера $N$ , где $N$ — количество объектов в выборке, $D$ — размер признакового пространства. Подразумевается, что первый столбец выборки объектов соответствует признаку для смещения и равен единице.\n",
    "\n",
    "- **``losses.py``** (1 point)\n",
    "  - класс в этом модуле задаёт конкретную функцию потерь, которую можно использовать для обучения линейной модели. Обратите внимание на то, что подсчёт всех функций может быть полностью векторизован (т.е. их можно реализовать без циклов). Предложенная в задании функция потерь должна поддерживать использование $l2$-регуляризации. Обратите внимание, что признак для смещения **не** должен учитываться в регуляризаторе.\n",
    "  - Класс должен поддерживать как плотные матрицы (``numpy.ndarray``), так и разреженные матрицы (``scipy.sparse.csr_matrix``). Класс ``LinearLoss`` наследуется от абстрактного класса BaseLoss и реализует два метода: ``func`` и ``grad``.\n",
    "    - ``func(self, X, y, w)`` — вычисление значения функции потерь на матрице признаков $X$, векторе ответов $y$ с вектором весов $w$.\n",
    "    - ``grad(self, X, y, w)`` — вычисление значения градиента функции потерь на матрице признаков $X$, векторе ответов $y$ с вектором весов $w$.\n",
    "  - У обоих методов одинаковые аргументы:\n",
    "     - $X$ - выборка объектов\n",
    "     - $y$ - вектор ответов\n",
    "     - $w$ - вектор коэффициентов модели ``numpy.ndarray``.\n",
    "\n",
    "В данном задании предлагается реализовать следующую функцию потерь:\n",
    "\n",
    "$$L(X,y,w, \\lambda) = \\frac{1}{N} \\| Xw - y \\|_2^2 + \\lambda \\|w\\|_2^2$$\n",
    "\n",
    "- **``linear_model.py``** (2 points)\n",
    "  - модуль с реализацией линейной модели, поддерживающей обучение через полный и стохастический градиентные спуски. Линейная модель должна задаваться в классе LinearModel. Параметр $\\eta_k > 0$ — темп обучения (learning rate) для градиентного спуска, где $k$ — номер эпохи, должен параметризовываться формулой: $\\eta_k = \\dfrac{\\alpha}{k^{\\beta}}, \\ где \\ \\ \\alpha, \\beta \\ — \\ \\ заданные \\ константы, \\ \\ k \\ — \\ \\ номер \\ итерации$\n",
    "\n",
    "  - Описание методов класса:\n",
    "    - ``__init__`` — конструктор (инициализатор) класса с параметрами:\n",
    "      - ``loss_function`` — функция потерь, заданная классом, наследованным от ``BaseLoss``\n",
    "      - ``batch_size`` — размер подвыборки, по которой считается градиент, если ``None``, то необходимо использовать полный градиент\n",
    "      - ``step_alpha`` — параметр выбора шага градиентного спуска\n",
    "      - ``step_beta`` — параметр выбора шага градиентного спуска\n",
    "      - ``tolerance`` — точность, по достижении которой, необходимо прекратить оптимизацию.\n",
    "        - **В данном случае в качестве критерия останова требуется реализовать вариант:** $\\| w_{current} - w_{previous} \\| \\le tolerance$\n",
    "      - ``max_iter`` — максимальное число итераций \n",
    "    - ``fit(self, X, y, w_0=None, trace=False)`` — обучение линейной модели:\n",
    "      - ``X`` - выборка объектов\n",
    "      - ``y`` - вектор ответов\n",
    "      - ``w_0`` - начальное приближение вектора весов, если ``None``, то необходимо инициализировать внутри метода\n",
    "      - ``trace`` - индикатор, нужно ли возвращать информацию об обучении\n",
    "        - Если ``trace is True``, то метод должен вернуть словарь ``history``, содержащий информацию о поведении метода оптимизации во время обучения. Длина словаря ``history`` — количество эпох. Элементы словаря в случае полного градиентного спуска:\n",
    "          - ``history[’time’]`` — содержит время, потраченное на обучение каждой эпохи\n",
    "          - ``history[’func’]`` — содержит значения функционала на обучающей выборке на каждой эпохе\n",
    "          - ``history[’func_val’]`` — содержит значения функционала на валидационной выборке на каждой эпохе\n",
    "        - Обратите внимание, что ``trace is True`` сильно замедляет обучение методов, т.к. требует в конце эпохи подсчитывать значение функции. Не используйте его ни в каких экспериментах, кроме экспериментов, где необходимо исследовать поведение функции в зависимости от гиперпараметров.\n",
    "        - Нет необходимости проводить честное семплирование для каждого батча в методе стохастического градиентного спуска. Вместо этого предлагается в начале одной эпохи сгенерировать случайную перестановку индексов объектов, а затем последовательно выбирать объекты для нового батча из элементов этой перестановки.\n",
    "    - ``predict(self, X)`` — получение предсказаний модели\n",
    "      - ``X`` - выборка объектов\n",
    "      - $\\rightarrow$ Метод должен вернуть ``numpy.ndarray`` такого же размера, как и первая размерность матрицы ``X``.\n",
    "    - ``get_objective(self, X, y)`` — вычисление значения функции потерь\n",
    "      - ``X`` - выборка объектов\n",
    "      - ``y`` - вектор ответов\n",
    "      - $\\rightarrow$ Функция должна вернуть вещественное число.\n",
    "    - ``get_weights(self)`` — получить вектор линейных коэффициентов модели\n",
    "\n",
    "--------------------------\n",
    "\n",
    "### Эксперименты. (13 points)\n",
    "\n",
    "Данные для этого задания находятся в файле ``hw3_data.csv``. Данные состоят из двух колонок: ``text`` и ``y``. Текст представляет собой комментарии пользователей. А целевая переменная, колонка ``y``, отражает степень токсичности комментария, которую вам необходимо будет предсказать.\n",
    "\n",
    "**1.** (2 points)\n",
    "\n",
    "*1.1* Произведите предварительную обработку текста. Приведите все тексты к нижнему регистру. Замените в тексте все символы, не являющиеся буквами и цифрами, на пробелы. Примените алгоритм [лемматизации](https://ru.wikipedia.org/wiki/%D0%9B%D0%B5%D0%BC%D0%BC%D0%B0%D1%82%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F) (например, [WordNetLemmatizer](https://www.nltk.org/api/nltk.stem.WordNetLemmatizer.html?highlight=wordnet) из библиотеки [nltk](https://www.nltk.org/index.html)) к коллекции. Удалите из текста стоп-слова (например, используя список стоп-слов из nltk).\n",
    "\n",
    "Замечание. Полезные функции: ``str.lower``, ``str.split``, ``str.isalnum``, ``re.sub``, ``re.split``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf322658-343e-465b-bdaa-a739e75571d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46fe4a01-d4dd-41af-9dcc-8a14fac12e4d",
   "metadata": {},
   "source": [
    "*1.2.* Разделите данные на обучение, валидацию и тест. Для теста выберете $30\\%$ __случайных__ объектов из датасета. Оставшиеся данные разбейте в соотношении $70/30$ (обучение/валидация). Рекомендуется использовать функцию ``sklearn.model_selection.train_test_split`` c параметром ``random_state=42``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e84833-9cc6-4025-94ec-7a7c7e3f3856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bc361fe-61dc-4615-b1e4-51d17248abd1",
   "metadata": {},
   "source": [
    "*1.3.* Преобразуйте текст в разреженную матрицу ``scipy.sparse.csr_matrix``, где значение $x$ в позиции $(i, j)$ сответствует [$tf-idf$](https://ru.wikipedia.org/wiki/TF-IDF) характеристке $j$-го слова в $i$-ом документе. Рекомендуется использовать конструктор [``sklearn.feature_extraction.text.TfidfVectorizer``](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Добавьте в данные единичный столбец на нулевой позиции.\n",
    "\n",
    "Замечание 1. У ``TfidfVectorizer`` есть несколько методов для работы, используйте ``fit_transform`` и ``fit`` для обучающей выборки, используйте ``transform`` для тестовой.\n",
    "\n",
    "Замечание 2. Используйте параметр ``min_df``, чтобы уменьшить размерность данных и ускорить проведение экспериментов. Рекомендуется использовать ``min_df`` не меньше 5.\n",
    "\n",
    "Замечание 3. Для добавления единичного столбца, можно воспользоваться следующей инструкцией: ``from scipy.sparse import hstack, csr_matrix\n",
    "X = csr_matrix(hstack([csr_matrix(np.ones((X.shape[0], 1))), X]))``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d8b34-f899-47f8-b4f7-bb094da34a38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed8a2ba1-b7fc-469f-91ff-978d07092b85",
   "metadata": {},
   "source": [
    "**2.** (3 points) В спецификации предлагается использовать следующую формулу для выбора темпа обучения $\\gamma_k$:\n",
    "\n",
    "$$\\gamma_k = \\dfrac{\\alpha}{k^{\\beta}}, \\ где \\ \\ \\alpha, \\beta \\ — \\ заданные \\ константы, \\ \\ k \\ — \\ номер \\ итерации$$\n",
    "\n",
    "   - Исследуйте поведение градиентного спуска для задачи линейной регрессии в зависимости от следующих параметров:\n",
    "        - параметр темпа обучения ``step_alpha``\n",
    "        - параметр темпа обучения ``step_beta``\n",
    "\n",
    "   - Исследование поведения подразумевает анализ следующих зависимостей на обучающей и валидационной выборках:\n",
    "        - зависимость значения функции потерь от реального времени работы метода\n",
    "        - зависимость значения функции потерь от эпохи метода\n",
    "        - значение метрики качества после обучения метода\n",
    "\n",
    "В качестве метрики качества здесь и далее предлагается использовать **MAE**.\n",
    "\n",
    "***Дисклеймер:*** *это исследовательская часть задания, где вы сами решаете, как много экспериментов проводить, как ограничить пространство поиска и прочие параметры. Оценка качества экспериментов будет основана на субъективных ощущениях проверяющего. Таким образом моделируется реальное исследование, когда вы в большинстве случаев не можете оценить, в какой области находится достаточно хорошее решение и сколько потребуется экспериментов для его достижения.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aeb754-e0ff-463e-adb8-27a81068fb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.linear_model import LinearModel\n",
    "from modules.losses import LinearLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca0fc48-b348-45b9-bf72-bac83fd3ae1d",
   "metadata": {},
   "source": [
    "**5.** (3 points)\n",
    "- Исследуйте поведение стохастического градиентного спуска для задачи линейной регрессии в зависимости от следующих параметров:\n",
    "  - параметр темпа обучения ``step_alpha``\n",
    "  - параметр темпа обучения ``step_beta``\n",
    "  - размер подвыборки ``batch_size``\n",
    "\n",
    "Замечание. Обратите внимание, что в стохастическом случае необходимо строить зависимости метрик качества от эпохи метода. За одну эпоху через оптимизацию модели проходит $N$ объектов, где $N$ — длина обучающей выборки. Если вы реализуете семплирование согласно спецификации задания, то за одну эпоху каждый объект пройдёт через оптимизацию ровно один раз. В полном градиентном спуске одна эпоха метода соответствует одной итерации обучения.\n",
    "\n",
    "***Дисклеймер:*** *это исследовательская часть задания, где вы сами решаете, как много экспериментов проводить, как ограничить пространство поиска и прочие параметры. Оценка качества экспериментов будет основана на субъективных ощущениях проверяющего. Таким образом моделируется реальное исследование, когда вы в большинстве случаев не можете оценить, в какой области находится достаточно хорошее решение и сколько потребуется экспериментов для его достижения.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87e260-7e11-4573-ad67-b8b3963b7c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "743cd09b-fc13-43af-b106-f2ffa70d61ce",
   "metadata": {},
   "source": [
    "**6.** (2 point) Сравните поведение двух методов между собой, сделайте выводы. Сравните оптимальные ``step_alpha`` и ``step_beta`` для разных методов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea7527-5b84-47c3-814f-c5103d2c8b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c04eb009-7045-425d-acae-1f9a4c05c441",
   "metadata": {},
   "source": [
    "**7.** (1 point) Подберите по отложенной выборке коэффициент $l2$-регуляризации модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961e39dc-ca9e-4826-baf7-82ecb5fea66c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5557a84e-9377-4c11-9428-995700331a0d",
   "metadata": {},
   "source": [
    "**8.** (2 points) Выберите лучший алгоритм для тестовой выборки. Проанализируйте ошибки алгоритма. Проанализируйте и укажите общие черты объектов, на которых были допущены ошибки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2803cc0-674e-427c-9123-c2cc59c346aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
